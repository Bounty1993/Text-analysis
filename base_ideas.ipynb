{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 2, 5, 6]\n",
      "[1, 7, 8, 9, 1, 10]\n",
      "[11, 12]\n",
      "[3, 13, 14, 15, 16]\n",
      "[3, 17, 18, 2, 19, 20, 21, 22, 1, 23, 24]\n",
      "Unique worlds: 24\n"
     ]
    }
   ],
   "source": [
    "samples = ['The cat is really beautiful.', \n",
    "           'The other side of the world',\n",
    "          'Hello Boy!',\n",
    "          'I hope you like me.',\n",
    "          'I think it is too late to find the correct way.']\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(samples)\n",
    "seq = tokenizer.texts_to_sequences(samples)\n",
    "for s in seq:\n",
    "    print(s)\n",
    "print(f'Unique worlds: {len(tokenizer.word_index)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  4,  2,  5,  6,  0,  0,  0,  0,  0],\n",
       "       [ 1,  7,  8,  9,  1, 10,  0,  0,  0,  0],\n",
       "       [11, 12,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 3, 13, 14, 15, 16,  0,  0,  0,  0,  0],\n",
       "       [17, 18,  2, 19, 20, 21, 22,  1, 23, 24]], dtype=int32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seq = pad_sequences(seq, 10, padding='post')\n",
    "input_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  4,  2,  5,  6,  0,  0,  0,  0,  0],\n",
       "       [ 1,  7,  8,  9,  1, 10,  0,  0,  0,  0],\n",
       "       [11, 12,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 3, 13, 14, 15, 16,  0,  0,  0,  0,  0],\n",
       "       [ 3, 17, 18,  2, 19, 20, 21, 22,  1, 23]], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequences(seq, 10, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bartosz/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/datasets/imdb.py:128: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/home/bartosz/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/datasets/imdb.py:129: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "max_feat = 10_000\n",
    "maxlen = 200\n",
    "(train_X, train_Y), (test_X, test_Y) = imdb.load_data(num_words=max_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/bartosz/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/bartosz/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 200, 8)            80000     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 1601      \n",
      "=================================================================\n",
      "Total params: 81,601\n",
      "Trainable params: 81,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "train_X = pad_sequences(train_X, maxlen=maxlen, padding='post')\n",
    "test_X = pad_sequences(test_X, maxlen=maxlen, padding='post')\n",
    "\n",
    "input_layer = layers.Input(shape=(maxlen))\n",
    "emb_layer = layers.Embedding(max_feat, 8)(input_layer)\n",
    "emb_layer = layers.Flatten()(emb_layer)\n",
    "output = layers.Dense(1, activation='sigmoid')(emb_layer)\n",
    "model = models.Model(input_layer, output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/bartosz/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "25000/25000 [==============================] - 1s 44us/sample - loss: 0.4447 - acc: 0.8580\n",
      "Test Accuracy: 0.8580399751663208\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_X, train_Y, \n",
    "          epochs=10, \n",
    "          batch_size=32,\n",
    "          verbose=0,\n",
    "          validation_split=0.2)\n",
    "scores = model.evaluate(test_X, test_Y)\n",
    "print(f'Test Accuracy: {scores[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_train = os.path.join('datasets', 'imdb', 'train')\n",
    "data = []\n",
    "labels = []\n",
    "for cat in ['pos', 'neg']:\n",
    "    tar_dir = os.path.join(imdb_train, cat)\n",
    "    files = os.listdir(tar_dir)\n",
    "    for file in files:\n",
    "        if os.path.splitext(file)[1] == '.txt':\n",
    "            file_path = os.path.join(tar_dir, file)\n",
    "            with open(file_path) as f:\n",
    "                data.append(f.read())\n",
    "            if cat == 'pos':\n",
    "                labels.append(1)\n",
    "            else:\n",
    "                labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of review: 1\n",
      "\n",
      "I was amazed at the improvements made in an animated film. If you sit close to the screen, you will see the detail in the grass and surface structures. The detail, colors, and shading are at least an order of magnitude better than Toy Story. How they were able to pull off the shading, I will never know. I do hope that PIXAR will provide a documentary on how the film was produced so I can find out how all this was accomplished. Based on this film, I think animated films of the future will be judged on the basis of this film.\n"
     ]
    }
   ],
   "source": [
    "print('Type of review:', labels[0])\n",
    "print()\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(data, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 10_000\n",
    "maxlen = 200\n",
    "\n",
    "imdb_tokenizer = Tokenizer(num_words=num_words)\n",
    "imdb_tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = imdb_tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = imdb_tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_seq = pad_sequences(X_train_seq, maxlen=maxlen, \n",
    "                            padding='post')\n",
    "X_test_seq = pad_sequences(X_test_seq, maxlen=maxlen,\n",
    "                           padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words  400000\n"
     ]
    }
   ],
   "source": [
    "emb_index = {}\n",
    "emb_path = os.path.join('embeddings', 'glove.6B.100d.txt')\n",
    "with open(emb_path) as f:\n",
    "    for row in f:\n",
    "        line = row.split()\n",
    "        word = line[0]\n",
    "        vec = line[1:]\n",
    "        emb_index[word] = np.array(vec)\n",
    "print('Number of words ', len(emb_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_len = len(emb_index['the'])\n",
    "emb_matrix = np.zeros(shape=(num_words, emb_len))\n",
    "for word, i in imdb_tokenizer.word_index.items():\n",
    "    if i < maxlen:\n",
    "        emb_vec = emb_index.get(word)\n",
    "        if emb_vec is not None:\n",
    "            emb_matrix[i] = emb_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 200, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 20000)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                640032    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,641,121\n",
      "Trainable params: 1,641,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(num_words, emb_len, input_length=maxlen))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([emb_matrix])\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 6s 294us/sample - loss: 0.6783 - acc: 0.5733 - val_loss: 0.6599 - val_acc: 0.6050\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 5s 267us/sample - loss: 0.6155 - acc: 0.6597 - val_loss: 0.6558 - val_acc: 0.6268\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 5s 270us/sample - loss: 0.5617 - acc: 0.7099 - val_loss: 0.6641 - val_acc: 0.6244\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 5s 271us/sample - loss: 0.5168 - acc: 0.7427 - val_loss: 0.6827 - val_acc: 0.6246\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 5s 270us/sample - loss: 0.4772 - acc: 0.7675 - val_loss: 0.6772 - val_acc: 0.6412\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 5s 268us/sample - loss: 0.4380 - acc: 0.7937 - val_loss: 0.6836 - val_acc: 0.6264\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 5s 270us/sample - loss: 0.4055 - acc: 0.8082 - val_loss: 0.7891 - val_acc: 0.6318\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 5s 272us/sample - loss: 0.3667 - acc: 0.8256 - val_loss: 0.8164 - val_acc: 0.6398\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 5s 273us/sample - loss: 0.3261 - acc: 0.8456 - val_loss: 0.9047 - val_acc: 0.6332\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 5s 271us/sample - loss: 0.2997 - acc: 0.8613 - val_loss: 1.0400 - val_acc: 0.6288\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train_seq, Y_train,\n",
    "                    epochs=10, batch_size=32,\n",
    "                    validation_data=(X_test_seq, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
